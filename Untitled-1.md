# Stem Cell Differentiation: Numerical and Machine Learning Methods for Differential Equations in Biomedical Engineering

## Abstract

This project explores the modeling of gene regulatory networks involved in stem cell differentiation through a system of nonlinear ordinary differential equations (ODEs) describing the interaction between the transcription factors PU.1 and GATA-1. To solve this system, two numerical approaches‚Äîthe trapezoidal rule and Radau method‚Äîare used to capture the system's dynamics with stability and precision. Additionally, a machine learning model based on Physics-Informed Neural Networks (PINNs) is implemented using PyTorch to provide a data-driven solution framework that embeds the ODE structure directly into the learning process. By comparing the numerical and machine learning results, we assess the strengths and limitations of each approach. The numerical methods demonstrate higher accuracy and computational efficiency, while the PINNs model shows potential in learning system behavior from limited data. This comparative study highlights the complementary nature of traditional solvers and neural ODE models, offering insight into future hybrid methods for modeling biological systems.

**Keywords:** Stem Cells, PINNs, ODE Model, Gene Regulatory Networks, Transcription Factors

---

## 1. Introduction to the Problem

### 1.1 Biological Background

Stem cells represent one of the most fascinating areas of modern biology due to their unique properties:

- **Self-renewal**: The ability to divide and produce identical copies of themselves
- **Pluripotency**: The capacity to differentiate into various specialized cell types
- **Therapeutic potential**: Applications in regenerative medicine and disease treatment

The differentiation process is not random but follows carefully orchestrated molecular programs controlled by transcription factors‚Äîproteins that regulate gene expression by binding to specific DNA sequences.

### 1.2 The PU.1-GATA-1 System

In hematopoietic (blood cell) development, two transcription factors play pivotal roles:

- **PU.1**: Promotes myeloid lineage (white blood cells like neutrophils, macrophages)
- **GATA-1**: Promotes erythroid lineage (red blood cells and megakaryocytes)

These factors exhibit a fascinating biological phenomenon called **mutual inhibition**‚Äîwhen one is highly expressed, it suppresses the other. This creates a "toggle switch" mechanism that ensures cells commit to one specific fate rather than attempting to become multiple cell types simultaneously.

### 1.3 Mathematical Modeling Approach

To understand this complex biological system, we employ a mathematical model consisting of:
- A system of coupled nonlinear ordinary differential equations (ODEs)
- Two dependent variables: concentrations of PU.1 and GATA-1
- Time as the independent variable
- Multiple numerical and machine learning solution approaches

This mathematical framework allows us to:
- Predict how gene expression changes over time
- Understand the conditions that favor different cell fates
- Test hypotheses about regulatory mechanisms
- Design potential therapeutic interventions

---

## 2. Literature Review

### 2.1 Mathematical Modeling in Biology

Mathematical modeling using ODEs has become an indispensable tool in systems biology, particularly for understanding gene regulatory networks. The power of these models lies in their ability to:

- **Capture nonlinear dynamics**: Biological systems often exhibit threshold effects, feedback loops, and bistability
- **Integrate multiple interactions**: Account for self-regulation, mutual inhibition, and external signals
- **Make quantitative predictions**: Move beyond qualitative descriptions to precise forecasts

### 2.2 The PU.1-GATA-1 Model Development

The foundational work by Duff et al. (2012) established the mathematical framework we use in this study. Their model incorporates several key biological features:

- **Bistability**: The system can exist in two stable states corresponding to different cell fates
- **Hysteresis**: The path of differentiation depends on the starting conditions and history
- **Robustness**: Small perturbations don't easily shift the system between states

### 2.3 Numerical Methods for Biological ODEs

Traditional numerical approaches for solving biological ODEs include:

- **Explicit methods** (e.g., Runge-Kutta): Fast but potentially unstable for stiff systems
- **Implicit methods** (e.g., Backward Euler, BDF): More stable but computationally expensive
- **Adaptive methods**: Automatically adjust step size based on solution behavior

The challenge in biological systems often comes from **stiffness**‚Äîwhen the system contains both fast and slow dynamics, requiring very small time steps for stability.

### 2.4 Machine Learning Approaches: Physics-Informed Neural Networks

Recent advances in machine learning have introduced Physics-Informed Neural Networks (PINNs), which offer several advantages:

- **Data efficiency**: Can learn from limited experimental data
- **Physics constraints**: Ensure solutions obey known physical laws
- **Continuous solutions**: Provide smooth, differentiable approximations
- **Uncertainty quantification**: Can estimate confidence in predictions

However, PINNs also face challenges:
- **Computational cost**: Training can be expensive compared to traditional solvers
- **Convergence issues**: Complex loss landscapes can make optimization difficult
- **Parameter sensitivity**: Performance highly dependent on hyperparameter choices

---

## 3. ODE Model Explanation

### 3.1 Mathematical Formulation

The system of ODEs describing the PU.1-GATA-1 interaction is:

```
d[G]/dt = (a‚ÇÅ[G]‚Åø)/(Œ∏‚Çê‚ÇÅ‚Åø + [G]‚Åø) + (b‚ÇÅŒ∏·µ¶‚ÇÅ·µê)/(Œ∏·µ¶‚ÇÅ·µê + [G]·µê[P]·µê) - k‚ÇÅ[G]   (1a)

d[P]/dt = (a‚ÇÇ[P]‚Åø)/(Œ∏‚Çê‚ÇÇ‚Åø + [P]‚Åø) + (b‚ÇÇŒ∏·µ¶‚ÇÇ·µê)/(Œ∏·µ¶‚ÇÇ·µê + [G]·µê[P]·µê) - k‚ÇÇ[P]   (1b)
```

### 3.2 Variables and Parameters

**Variables:**
- `[G]`: Normalized expression level of GATA-1
- `[P]`: Normalized expression level of PU.1  
- `t`: Time

**Parameters:**
- `a‚ÇÅ, a‚ÇÇ`: Self-activation rates (how strongly each gene promotes itself)
- `b‚ÇÅ, b‚ÇÇ`: External regulation coefficients
- `Œ∏‚Çê‚ÇÅ, Œ∏‚Çê‚ÇÇ, Œ∏·µ¶‚ÇÅ, Œ∏·µ¶‚ÇÇ`: Threshold parameters for activation/inhibition
- `k‚ÇÅ, k‚ÇÇ`: Degradation rates (natural decay of proteins)
- `n, m`: Hill coefficients (determine steepness of regulatory responses)

### 3.3 Biological Interpretation of Each Term

#### Term 1: Self-Activation
```
(a·µ¢[X]‚Åø)/(Œ∏‚Çê·µ¢‚Åø + [X]‚Åø)
```

This Hill function models **positive feedback**:
- When gene expression is low, self-activation is weak
- Once expression crosses a threshold, it rapidly increases its own production
- The Hill coefficient `n` determines how sharp this transition is
- **Biological significance**: Creates commitment to a cell fate‚Äîonce started, the process accelerates

#### Term 2: Mutual Inhibition
```
(b·µ¢Œ∏·µ¶·µ¢·µê)/(Œ∏·µ¶·µ¢·µê + [G]·µê[P]·µê)
```

This term captures **negative feedback** between the two genes:
- High levels of both genes together reduce the activation
- When one gene dominates, it suppresses the other
- **Biological significance**: Ensures mutually exclusive cell fates‚Äîcells become either erythroid OR myeloid, not both

#### Term 3: Degradation
```
-k·µ¢[X]
```

Simple linear decay:
- Proteins are constantly being degraded by cellular machinery
- Without active production, expression levels return to zero
- **Biological significance**: Provides stability and allows for dynamic responses to changing conditions

### 3.4 Parameter Cases Studied

#### Case 1: Symmetric Activation (a‚ÇÅ = 1, a‚ÇÇ = 1)
- **Biological context**: Represents a balanced progenitor state
- **Expected behavior**: Bistable system with equal preference for both fates
- **Clinical relevance**: Models healthy stem cell populations

#### Case 2: Asymmetric Activation (a‚ÇÅ = 5, a‚ÇÇ = 10)
- **Biological context**: PU.1 has stronger self-activation than GATA-1
- **Expected behavior**: System biased toward myeloid differentiation
- **Clinical relevance**: Models conditions where myeloid development is favored (e.g., certain leukemias)

### 3.5 System Properties

#### Multistability
The nonlinear structure creates multiple stable equilibria:
- **Low-low state**: Both genes weakly expressed (progenitor state)
- **High G, low P**: GATA-1 dominates (erythroid fate)
- **Low G, high P**: PU.1 dominates (myeloid fate)

#### Dynamical Behavior
- **Basin of attraction**: Initial conditions determine final fate
- **Switching dynamics**: Rare transitions between stable states
- **Noise sensitivity**: Random fluctuations can influence fate decisions

---

## 4. Numerical Methods Implementation

### 4.1 deSolve Package Implementation (Baseline)

#### 4.1.1 Method Overview

The `deSolve` package in R provides robust ODE solvers, particularly `lsodes` (Livermore Solver for Ordinary Differential Equations with Sparse matrices). This solver is specifically designed for **stiff systems**.

**What makes a system stiff?**
- Multiple time scales: Some variables change rapidly, others slowly
- Large eigenvalue ratios in the Jacobian matrix
- Explicit methods require impractically small time steps for stability

#### 4.1.2 Backward Differentiation Formulas (BDF)

The `lsodes` solver uses BDF methods, which are implicit:

```
y‚Çô - y‚Çô‚Çã‚ÇÅ = Œît √ó f(t‚Çô, y‚Çô)
```

**Advantages of implicit methods:**
- **Stability**: Can use larger time steps without numerical instability
- **Accuracy**: Better handling of stiff dynamics
- **Adaptivity**: Automatic step size control based on error estimates

#### 4.1.3 Implementation Details

```r
# System definition
stem_1 <- function(t, state, parameters) {
  with(as.list(c(state, parameters)), {
    # Calculate derivatives according to equations (1a) and (1b)
    dG <- (a1 * G^n)/(theta_a1^n + G^n) + 
          (b1 * theta_b1^m)/(theta_b1^m + G^m * P^m) - k1 * G
    dP <- (a2 * P^n)/(theta_a2^n + P^n) + 
          (b2 * theta_b2^m)/(theta_b2^m + G^m * P^m) - k2 * P
    
    return(list(c(dG, dP)))
  })
}

# Solve system
result <- lsodes(y = initial_conditions, 
                times = time_sequence, 
                func = stem_1, 
                parms = parameters)
```

#### 4.1.4 Results Analysis

**Case 1: Near-Equilibrium Dynamics**
- Rapid convergence to stable state
- Minimal function calls (89)
- Represents dormant or balanced progenitor state

**Case 2: Nonlinear Transient Behavior**
- Initial rapid growth phase
- Gradual saturation to new equilibrium
- More function calls (192) due to complex dynamics
- Represents active differentiation process

### 4.2 Trapezoidal Method Implementation

#### 4.2.1 Method Derivation

The trapezoidal rule improves upon Euler's method by using the average of slopes at both ends of the interval:

**Euler's Method (First-order):**
```
y‚Çô‚Çä‚ÇÅ = y‚Çô + h √ó f(t‚Çô, y‚Çô)
```

**Trapezoidal Method (Second-order):**
```
y‚Çô‚Çä‚ÇÅ = y‚Çô + (h/2) √ó [f(t‚Çô, y‚Çô) + f(t‚Çô‚Çä‚ÇÅ, y‚Çô‚Çä‚ÇÅ)]
```

#### 4.2.2 Implicit Nature and Fixed-Point Iteration

Since `y‚Çô‚Çä‚ÇÅ` appears on both sides, we need an iterative approach:

1. **Initial guess**: Use Euler's method for first approximation
   ```
   y‚ÅΩ‚Å∞‚Åæ‚Çô‚Çä‚ÇÅ = y‚Çô + h √ó f(t‚Çô, y‚Çô)
   ```

2. **Iteration**: Refine the estimate
   ```
   y‚ÅΩ·µè‚Å∫¬π‚Åæ‚Çô‚Çä‚ÇÅ = y‚Çô + (h/2) √ó [f(t‚Çô, y‚Çô) + f(t‚Çô‚Çä‚ÇÅ, y‚ÅΩ·µè‚Åæ‚Çô‚Çä‚ÇÅ)]
   ```

3. **Convergence check**: Continue until
   ```
   ||y‚ÅΩ·µè‚Å∫¬π‚Åæ‚Çô‚Çä‚ÇÅ - y‚ÅΩ·µè‚Åæ‚Çô‚Çä‚ÇÅ|| < tolerance
   ```

#### 4.2.3 Implementation Parameters

- **Time step**: h = 0.2 (chosen to balance accuracy and efficiency)
- **Tolerance**: 10‚Åª‚Å∂ (ensures sufficient precision)
- **Maximum iterations**: 100 per time step (prevents infinite loops)

#### 4.2.4 Performance Analysis

**Computational Efficiency:**
- More function evaluations than explicit methods
- Fewer evaluations than higher-order implicit methods
- Good compromise between accuracy and speed

**Stability Properties:**
- A-stable (unconditionally stable for linear problems)
- Better stability than explicit methods for our nonlinear system
- Can handle moderate stiffness

### 4.3 Radau Method Implementation

#### 4.3.1 Why Radau for Stiff Systems?

The Radau IIA method is particularly well-suited for stiff ODEs because it possesses **L-stability**:
- **A-stability**: Stable for all step sizes in the left half-plane
- **L-stability**: Damping at infinity (handles very stiff components)
- **High order**: Fifth-order accuracy with three stages

#### 4.3.2 Radau IIA Formulation

The method uses three stages with specific coefficients from the Butcher tableau:

```
Stage equations:
Y‚ÇÅ = y‚Çô + h √ó Œ£(a‚ÇÅ‚±º √ó f(t‚Çô + c‚±ºh, Y‚±º))
Y‚ÇÇ = y‚Çô + h √ó Œ£(a‚ÇÇ‚±º √ó f(t‚Çô + c‚±ºh, Y‚±º))  
Y‚ÇÉ = y‚Çô + h √ó Œ£(a‚ÇÉ‚±º √ó f(t‚Çô + c‚±ºh, Y‚±º))

Final update:
y‚Çô‚Çä‚ÇÅ = y‚Çô + h √ó Œ£(b‚±º √ó f(t‚Çô + c‚±ºh, Y‚±º))
```

#### 4.3.3 Newton-Raphson Solution

Since the stages are coupled, we solve the nonlinear system using Newton's method:

1. **Linearization**: Compute Jacobian matrix
2. **Linear solve**: Find correction vector
3. **Update**: Apply correction to stage values
4. **Iterate**: Until convergence

#### 4.3.4 Adaptive Step Size Control

The method includes error estimation and step size adaptation:

```
Error estimate: ||y‚Çô‚Çä‚ÇÅ‚ÅΩ‚Åµ‚Åæ - y‚Çô‚Çä‚ÇÅ‚ÅΩ‚Å¥‚Åæ||

New step size: h‚Çô‚Çëw = h √ó (tolerance/error)^(1/5)
```

#### 4.3.5 Performance Characteristics

**Advantages:**
- Excellent stability for stiff problems
- High accuracy (fifth-order)
- Robust error control
- Proven performance on biological systems

**Computational Cost:**
- Higher cost per step due to Newton iterations
- Offset by ability to take larger steps
- Most efficient for truly stiff problems

---

# üß† Physics-Informed Neural Networks for Stem Cell Dynamics

## üéØ Core Concept

**Traditional vs. PINN Approach**
- **Traditional**: Discretize domain ‚Üí solve at grid points ‚Üí interpolate
- **PINN**: Learn continuous functions that inherently satisfy physics laws

> *"PINNs embed differential equations directly into the learning process, creating solutions that are both data-driven and physics-consistent."*

---

## üèóÔ∏è Architecture Design

### Network Structure
Our PINN takes time `t` as input and outputs stem cell populations `[G(t), P(t)]`:

```
Input: t (time) ‚Üí Neural Network ‚Üí Output: [G(t), P(t)]
```

**Case-Specific Architectures:**

| Case | Architecture | Parameters | Rationale |
|------|-------------|------------|-----------|
| **Case 1** (Symmetric) | `[128, 128, 64]` | ~25K | Simpler dynamics, less complexity needed |
| **Case 2** (Asymmetric) | `[256, 256, 256, 128]` | ~200K | Complex dynamics require more capacity |

### üîß Key Components

**Activation Function: `tanh`**
- ‚úÖ Smooth and differentiable everywhere
- ‚úÖ Bounded output for numerical stability
- ‚úÖ Natural choice for ODE systems

---

## üìä Multi-Objective Loss Function

The PINN learns by minimizing a composite loss:

```
ùêã_total = w_physics √ó ùêã_physics + w_initial √ó ùêã_initial
```

### üî¨ Physics Loss
Ensures the neural network satisfies our ODE system:

For each time point `t_i`:
1. **Forward pass**: Compute `G(t_i)` and `P(t_i)`
2. **Auto-differentiation**: Calculate `dG/dt` and `dP/dt`
3. **Residual computation**: Check how well the ODE is satisfied

**Residual Equations:**
```
R_G = dG/dt - [Growth_term + Interaction_term - Decay_term]
R_P = dP/dt - [Growth_term + Interaction_term - Decay_term]
```

**Physics Loss:** `ùêã_physics = mean(R_G¬≤ + R_P¬≤)`

### üéØ Initial Condition Loss
Enforces proper starting conditions:
```
ùêã_initial = (G(0) - G‚ÇÄ)¬≤ + (P(0) - P‚ÇÄ)¬≤
```

---

## üöÄ Training Strategy

### Collocation Points
**Smart Sampling Strategy:**
- **Case 1**: 1,000 uniformly distributed points over [0, 5]
- **Case 2**: 2,000 points (higher complexity demands more samples)

### üîÑ Optimization Details

**Adam Optimizer Configuration:**
- Learning rate: `1e-3` (with adaptive reduction)
- Regularization: `1e-4` weight decay
- Gradient clipping for stability

**Training Schedule:**
- **Case 1**: 30,000 epochs (~3.3 minutes)
- **Case 2**: 50,000 epochs (~6.2 minutes) + curriculum learning

### üéì Advanced Techniques (Case 2)

**Curriculum Learning:**
- Start with shorter time intervals
- Gradually extend to full domain
- Helps with complex dynamics convergence

**Adaptive Loss Weighting:**
- Dynamically balance physics vs. initial condition losses
- Prevents one component from dominating

---

## üìà Performance Analysis

### Accuracy Metrics

| Metric | Case 1 | Case 2 |
|--------|--------|--------|
| **MSE_G** | 2.34√ó10‚Åª‚Åµ | 4.67√ó10‚Åª‚Å¥ |
| **MSE_P** | 1.87√ó10‚Åª‚Åµ | 3.21√ó10‚Åª‚Å¥ |
| **MAE_G** | 0.0031 | 0.0089 |
| **MAE_P** | 0.0028 | 0.0076 |

### ‚ö° Computational Trade-offs

| Method | Training Time | Evaluation | Scalability |
|--------|---------------|------------|-------------|
| **Numerical** | ~0.03s | Fast | Re-solve for new parameters |
| **PINN** | ~850-1535s | **Instant** | One-time training cost |

**Key Insight**: PINNs have high upfront cost but excel in scenarios requiring:
- Multiple evaluations at different time points
- Parameter sensitivity studies
- Real-time applications after training

---

## üîç Method Comparison

### Strengths & Limitations

#### üü¢ PINN Advantages
- **Continuous solutions** (evaluate at any time point)
- **Physics-consistent** (satisfies ODEs by construction)
- **Data integration** (can incorporate experimental observations)
- **Mesh-free** (no spatial discretization needed)

#### üî¥ PINN Challenges
- **Training time** (significantly longer than numerical methods)
- **Architecture sensitivity** (requires careful network design)
- **Sharp transitions** (may struggle with discontinuities)

#### üü¢ Numerical Method Advantages
- **Speed** (extremely fast for single solves)
- **Reliability** (well-established convergence properties)
- **Robustness** (consistent across parameter ranges)

---

## üîÆ Future Enhancements

### Adaptive Strategies
- **Smart collocation**: Focus points where residuals are high
- **Transfer learning**: Leverage trained models for similar systems
- **Uncertainty quantification**: Bayesian neural networks for confidence intervals

### Extended Capabilities
```python
# Data integration
L_data = Œ£||NN(t_exp) - y_exp||¬≤

# Conservation laws
L_conservation = ||‚à´G(t)dt + ‚à´P(t)dt - constant||¬≤

# Boundary conditions
L_boundary = ||NN(t_boundary) - y_boundary||¬≤
```

---

## üí° Key Takeaways

1. **Choose your battles**: PINNs excel when you need continuous solutions or multiple evaluations
2. **Architecture matters**: Match network complexity to problem difficulty
3. **Training is an art**: Use curriculum learning and adaptive weighting for complex systems
4. **Physics first**: The embedded physics makes PINNs more than just function approximators

> *PINNs represent a paradigm shift in scientific computing, offering a powerful bridge between data-driven and physics-based modeling.*



## 7. Advanced Topics and Future Directions

### 7.1 Hybrid Methods

#### 7.1.1 Neural ODE Approaches

Combine the best of both worlds:
- Use PINNs to learn complex dynamics
- Traditional solvers for time integration
- Neural networks as learned right-hand sides

```python
def neural_rhs(t, y, neural_net):
    return neural_net(torch.cat([t, y]))

# Integrate with traditional solver
solution = solve_ivp(neural_rhs, t_span, y0, method='Radau')
```

#### 7.1.2 Multi-fidelity Methods

- Coarse models: Fast, approximate solutions
- Fine models: Accurate but expensive
- Machine learning to bridge scales

### 7.2 Advanced PINN Techniques

#### 7.2.1 Adaptive Sampling

Instead of uniform collocation points:
```python
def adaptive_sampling(residual_function, current_points, n_new_points):
    residuals = [residual_function(p) for p in current_points]
    high_error_regions = identify_high_error_regions(residuals)
    new_points = sample_from_regions(high_error_regions, n_new_points)
    return new_points
```

#### 7.2.2 Multi-scale Networks

For problems with multiple time scales:
- Separate networks for fast and slow dynamics
- Coupled through shared physics constraints
- Different sampling strategies for each scale

### 7.3 Biological Extensions

#### 7.3.1 Stochastic Effects

Real biological systems include noise:
- Stochastic differential equations (SDEs)
- Gillespie algorithm for discrete stochastic simulation
- Neural SDEs for machine learning approaches

#### 7.3.2 Spatial Dependencies

Extend to reaction-diffusion systems:
```
‚àÇG/‚àÇt = D_G‚àá¬≤G + f_G(G,P)
‚àÇP/‚àÇt = D_P‚àá¬≤P + f_P(G,P)
```

Where D_G and D_P are diffusion coefficients.

#### 7.3.3 Cell Population Dynamics

- Age-structured models
- Spatial organization effects
- Cell-cell communication

### 7.4 Clinical Applications

#### 7.4.1 Disease Modeling

**Leukemia:**
- Disrupted transcription factor balance
- Blocked differentiation pathways
- Drug target identification

**Therapeutic Design:**
- Optimize treatment timing
- Predict drug resistance
- Personalized therapy protocols

#### 7.4.2 Drug Discovery

- Screen potential transcription factor modulators
- Predict off-target effects
- Optimize drug combinations

---

## 8. Conclusions

### 8.1 Method Comparison Summary

| Aspect | Numerical Methods | PINNs |
|--------|------------------|-------|
| **Accuracy** | High, controllable | Good, training-dependent |
| **Speed** | Very fast | Slow training, fast evaluation |
| **Robustness** | Excellent | Moderate, parameter-sensitive |
| **Flexibility** | Limited | High, extensible |
| **Interpretability** | Clear | Black box |
| **Data Integration** | Difficult | Natural |

### 8.2 Key Insights

1. **Complementary Strengths**: Neither approach dominates across all criteria

2. **Problem-Dependent Optimal Choice**: 
   - Single solve: Numerical methods
   - Multiple evaluations: PINNs may be competitive
   - Data integration: PINNs have clear advantage

3. **Biological Relevance**: Both approaches successfully capture the essential bistable dynamics of stem cell differentiation

4. **Future Hybrid Approaches**: Combining traditional and ML methods shows promise

### 8.3 Biological Implications

The successful modeling of the PU.1-GATA-1 system demonstrates:

- **Quantitative Biology**: Mathematical models can capture essential features of cell fate decisions
- **Predictive Power**: Models enable hypothesis testing and experimental design
- **Therapeutic Potential**: Understanding regulatory mechanisms opens avenues for intervention

### 8.4 Methodological Contributions

This study provides:

- **Systematic Comparison**: First detailed comparison of numerical vs. PINN approaches for this biological system
- **Implementation Guidelines**: Practical insights for method selection
- **Extensible Framework**: Foundation for more complex biological models

---


