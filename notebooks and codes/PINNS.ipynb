{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeb81fc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from scipy.integrate import solve_ivp\n",
    "from google.colab import files\n",
    "\n",
    "# Device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c8ac5e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Common parameters\n",
    "b1, b2 = 1, 1\n",
    "tha1, tha2 = 0.5, 0.5\n",
    "thb1, thb2 = 0.07, 0.07\n",
    "k1, k2 = 1, 1\n",
    "n, m = 4, 1\n",
    "\n",
    "# Time domain\n",
    "t_start, t_end = 0.0, 10.0\n",
    "n_points = 200\n",
    "t_eval = np.linspace(t_start, t_end, n_points)\n",
    "t_train = torch.linspace(t_start, t_end, n_points).view(-1, 1).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d21485a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# PINN Model\n",
    "class PINN(nn.Module):\n",
    "    def __init__(self, case_num):\n",
    "        super().__init__()\n",
    "        # Case-specific architecture\n",
    "        if case_num == 1:\n",
    "            # Simpler architecture for Case 1\n",
    "            self.net = nn.Sequential(\n",
    "                nn.Linear(1, 128),\n",
    "                nn.Tanh(),\n",
    "                nn.Linear(128, 128),\n",
    "                nn.Tanh(),\n",
    "                nn.Linear(128, 64),\n",
    "                nn.Tanh(),\n",
    "                nn.Linear(64, 2)\n",
    "            )\n",
    "        else:\n",
    "            # More complex architecture for Case 2\n",
    "            self.net = nn.Sequential(\n",
    "                nn.Linear(1, 256),\n",
    "                nn.Tanh(),\n",
    "                nn.Linear(256, 256),\n",
    "                nn.Tanh(),\n",
    "                nn.Linear(256, 256),\n",
    "                nn.Tanh(),\n",
    "                nn.Linear(256, 128),\n",
    "                nn.Tanh(),\n",
    "                nn.Linear(128, 2)\n",
    "            )\n",
    "\n",
    "        # Initialize weights using Xavier initialization\n",
    "        for m in self.net.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.xavier_normal_(m.weight)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "    def forward(self, t):\n",
    "        return self.net(t)\n",
    "\n",
    "# Physics Loss\n",
    "def ode_residuals(model, t, a1, a2):\n",
    "    t.requires_grad = True\n",
    "    G_pred, P_pred = model(t).split(1, dim=1)\n",
    "\n",
    "    dG_dt = torch.autograd.grad(G_pred, t, torch.ones_like(G_pred), create_graph=True)[0]\n",
    "    dP_dt = torch.autograd.grad(P_pred, t, torch.ones_like(P_pred), create_graph=True)[0]\n",
    "\n",
    "    f1 = (a1 * G_pred**n / (tha1**n + G_pred**n)) + (b1 * thb1**m / (thb1**m + G_pred**m * P_pred**m)) - k1 * G_pred\n",
    "    f2 = (a2 * P_pred**n / (tha2**n + P_pred**n)) + (b2 * thb2**m / (thb2**m + G_pred**m * P_pred**m)) - k2 * P_pred\n",
    "\n",
    "    res1 = dG_dt - f1\n",
    "    res2 = dP_dt - f2\n",
    "\n",
    "    return res1, res2, G_pred, P_pred\n",
    "\n",
    "def train_pinn(G0, P0, a1, a2, case_name):\n",
    "    print(f\"\\n=== Training PINN for {case_name} ===\")\n",
    "\n",
    "    # Determine case number from parameters\n",
    "    case_num = 2 if a1 > 1 else 1\n",
    "\n",
    "    model = PINN(case_num).to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "    # Case-specific learning rate scheduler\n",
    "    if case_num == 1:\n",
    "        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "            optimizer, mode='min', factor=0.5, patience=500,\n",
    "            verbose=True, min_lr=1e-5\n",
    "        )\n",
    "    else:\n",
    "        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "            optimizer, mode='min', factor=0.5, patience=1000,\n",
    "            verbose=True, min_lr=1e-6\n",
    "        )\n",
    "\n",
    "    loss_fn = nn.MSELoss()\n",
    "\n",
    "    start_time = time.time()\n",
    "    loss_history = []\n",
    "\n",
    "    # Case-specific number of epochs\n",
    "    n_epochs = 30000 if case_num == 1 else 50000\n",
    "\n",
    "    # Case-specific loss weights\n",
    "    w_phys = 1.0\n",
    "    w_ic = 10.0 if case_num == 1 else 5.0\n",
    "\n",
    "    # Curriculum learning parameters (only for Case 2)\n",
    "    curriculum_steps = 5 if case_num == 2 else 1\n",
    "    current_step = 0\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Curriculum learning only for Case 2\n",
    "        if case_num == 2 and epoch % (n_epochs // curriculum_steps) == 0 and current_step < curriculum_steps:\n",
    "            current_step += 1\n",
    "            t_end_curr = t_end * (current_step / curriculum_steps)\n",
    "            t_train_curr = torch.linspace(t_start, t_end_curr, n_points).view(-1, 1).to(device)\n",
    "        else:\n",
    "            t_train_curr = t_train\n",
    "\n",
    "        resG, resP, G_pred, P_pred = ode_residuals(model, t_train_curr, a1, a2)\n",
    "\n",
    "        # Case-specific loss computation\n",
    "        loss_phys = w_phys * (loss_fn(resG, torch.zeros_like(resG)) + loss_fn(resP, torch.zeros_like(resP)))\n",
    "\n",
    "        # Initial condition loss\n",
    "        t0 = torch.tensor([[0.0]], dtype=torch.float32, requires_grad=True).to(device)\n",
    "        G0_pred, P0_pred = model(t0).split(1, dim=1)\n",
    "        loss_ic = w_ic * (loss_fn(G0_pred, torch.tensor([[G0]], dtype=torch.float32, device=device)) +\n",
    "                          loss_fn(P0_pred, torch.tensor([[P0]], dtype=torch.float32, device=device)))\n",
    "\n",
    "        # Adaptive weighting only for Case 2\n",
    "        if case_num == 2 and epoch > 1000:\n",
    "            w_phys = min(1.0, loss_ic.item() / (loss_phys.item() + 1e-8))\n",
    "            w_ic = min(10.0, loss_phys.item() / (loss_ic.item() + 1e-8))\n",
    "\n",
    "        loss = loss_phys + loss_ic\n",
    "        loss.backward()\n",
    "\n",
    "        # Gradient clipping with case-specific thresholds\n",
    "        max_norm = 0.5 if case_num == 1 else 1.0\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=max_norm)\n",
    "\n",
    "        optimizer.step()\n",
    "        scheduler.step(loss)\n",
    "\n",
    "        loss_history.append(loss.item())\n",
    "\n",
    "        if epoch % 500 == 0:\n",
    "            print(f\"Epoch {epoch}, Loss: {loss.item():.6f}, LR: {optimizer.param_groups[0]['lr']:.6f}\")\n",
    "            print(f\"  Physics Loss: {loss_phys.item():.6f}, IC Loss: {loss_ic.item():.6f}\")\n",
    "            if case_num == 2:\n",
    "                print(f\"  Weights - Physics: {w_phys:.2f}, IC: {w_ic:.2f}\")\n",
    "\n",
    "    elapsed_time = time.time() - start_time\n",
    "    print(f\"Training completed in {elapsed_time:.2f} seconds.\")\n",
    "\n",
    "    # Evaluate\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        pred = model(t_train).cpu().numpy()\n",
    "    G_pred, P_pred = pred[:, 0], pred[:, 1]\n",
    "\n",
    "    return G_pred, P_pred, elapsed_time, loss_history\n",
    "results ={}\n",
    "for ncase in [1,2]:\n",
    "    if ncase == 1:\n",
    "            G0, P0, a1, a2 = 1, 1, 1, 1\n",
    "    elif ncase == 2:\n",
    "            G0, P0, a1, a2 = 1, 1, 5, 10\n",
    "    case_name = f\"Case {ncase} (a1={a1}, a2={a2})\"\n",
    "    G_pinn, P_pinn, time_pinn, loss_history = train_pinn(G0, P0, a1, a2, case_name)\n",
    "    results[ncase] = {\n",
    "        't': t_eval,\n",
    "        'G_pinn': G_pinn, 'P_pinn': P_pinn,\n",
    "        'time_pinn': time_pinn,\n",
    "        'loss_history': loss_history,\n",
    "        'params': f\"a1={a1}, a2={a2}\"\n",
    "    }\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
